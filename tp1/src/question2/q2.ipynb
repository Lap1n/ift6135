{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Convolutional Networks: Many techniques correspond to incorporating certain prior knowledge\n",
    "of the structure of the data into the parameterization of the model. Convolution operation, for\n",
    "example, is designed for visual imagery.\n",
    "\n",
    "\n",
    "Instructions: For this part of the assignment we will train a convolutional network on MNIST\n",
    "for 10 epochs using your favorite deep learning frameworks such as Pytorch of Tensorflow. Plot the\n",
    "train and valid errors at the end of each epoch for the model.\n",
    "1. Come up with a CNN architecture with more or less similar number of parameters as MLP\n",
    "trained in Problem 1 and describe it.\n",
    "2. Compare the performances of CNN vs MLP. Comment.\n",
    "You could take reference from the architecture mentioned here https://github.com/MaximumEntropy/welcome_tutorials/tree/pytorch/pytorch ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, transform=mnist_transforms, download=True)\n",
    "mnist_test = torchvision.datasets.MNIST(root='./data', train=False, transform=mnist_transforms, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True, num_workers=12)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=64, shuffle=True, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters : 583402\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(Block, self).__init__()\n",
    "        self.c = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, n_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n_channels, n_channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        self.relu = nn.ReLU(True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.c(x) + x)\n",
    "      \n",
    "class SuperBlock(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(SuperBlock, self).__init__()\n",
    "        self.out = nn.Sequential(\n",
    "            Block(n_channels),\n",
    "            Block(n_channels),\n",
    "            Block(n_channels),\n",
    "            Block(n_channels),\n",
    "            Block(n_channels))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.out(x)\n",
    "        \n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, 3), padding=1),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.out(x)\n",
    "        \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, out_n):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.resnet_fake = nn.Sequential(\n",
    "            ConvLayer(1,16),\n",
    "            SuperBlock(16),\n",
    "            ConvLayer(16,32),\n",
    "            SuperBlock(32),\n",
    "            ConvLayer(32,64),\n",
    "            SuperBlock(64),\n",
    "            ConvLayer(64,128)\n",
    "            )\n",
    "        \n",
    "        self.linear = nn.Linear(128,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet_fake(x)\n",
    "        return self.linear(x.squeeze())\n",
    "\n",
    "    \n",
    "cuda_available = torch.cuda.is_available()\n",
    "clf = Classifier(128)\n",
    "if cuda_available:\n",
    "    clf = clf.cuda()\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "number_of_parameters = count_parameters(clf)\n",
    "print('Number of parameters : %d' % (number_of_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Batch : 0, Loss : 2.351 \n",
      "Epoch : 0, Batch : 200, Loss : 1.553 \n",
      "Epoch : 0, Batch : 400, Loss : 1.050 \n",
      "Epoch : 0, Batch : 600, Loss : 0.816 \n",
      "Epoch : 0, Batch : 800, Loss : 0.678 \n",
      "Epoch : 0 Train Acc : 95.578\n",
      "Epoch : 0 Test Acc : 95.790\n",
      "--------------------------------------------------------------\n",
      "Epoch : 1, Batch : 0, Loss : 0.231 \n",
      "Epoch : 1, Batch : 200, Loss : 0.217 \n",
      "Epoch : 1, Batch : 400, Loss : 0.198 \n",
      "Epoch : 1, Batch : 600, Loss : 0.184 \n",
      "Epoch : 1, Batch : 800, Loss : 0.175 \n",
      "Epoch : 1 Train Acc : 97.315\n",
      "Epoch : 1 Test Acc : 97.420\n",
      "--------------------------------------------------------------\n",
      "Epoch : 2, Batch : 0, Loss : 0.048 \n",
      "Epoch : 2, Batch : 200, Loss : 0.137 \n",
      "Epoch : 2, Batch : 400, Loss : 0.129 \n",
      "Epoch : 2, Batch : 600, Loss : 0.130 \n",
      "Epoch : 2, Batch : 800, Loss : 0.127 \n",
      "Epoch : 2 Train Acc : 97.858\n",
      "Epoch : 2 Test Acc : 97.980\n",
      "--------------------------------------------------------------\n",
      "Epoch : 3, Batch : 0, Loss : 0.206 \n",
      "Epoch : 3, Batch : 200, Loss : 0.108 \n",
      "Epoch : 3, Batch : 400, Loss : 0.100 \n",
      "Epoch : 3, Batch : 600, Loss : 0.101 \n",
      "Epoch : 3, Batch : 800, Loss : 0.101 \n",
      "Epoch : 3 Train Acc : 98.385\n",
      "Epoch : 3 Test Acc : 98.440\n",
      "--------------------------------------------------------------\n",
      "Epoch : 4, Batch : 0, Loss : 0.074 \n",
      "Epoch : 4, Batch : 200, Loss : 0.088 \n",
      "Epoch : 4, Batch : 400, Loss : 0.090 \n",
      "Epoch : 4, Batch : 600, Loss : 0.084 \n",
      "Epoch : 4, Batch : 800, Loss : 0.085 \n",
      "Epoch : 4 Train Acc : 98.548\n",
      "Epoch : 4 Test Acc : 98.710\n",
      "--------------------------------------------------------------\n",
      "Epoch : 5, Batch : 0, Loss : 0.074 \n",
      "Epoch : 5, Batch : 200, Loss : 0.077 \n",
      "Epoch : 5, Batch : 400, Loss : 0.075 \n",
      "Epoch : 5, Batch : 600, Loss : 0.077 \n",
      "Epoch : 5, Batch : 800, Loss : 0.078 \n",
      "Epoch : 5 Train Acc : 98.700\n",
      "Epoch : 5 Test Acc : 98.730\n",
      "--------------------------------------------------------------\n",
      "Epoch : 6, Batch : 0, Loss : 0.005 \n",
      "Epoch : 6, Batch : 200, Loss : 0.062 \n",
      "Epoch : 6, Batch : 400, Loss : 0.062 \n",
      "Epoch : 6, Batch : 600, Loss : 0.063 \n",
      "Epoch : 6, Batch : 800, Loss : 0.066 \n",
      "Epoch : 6 Train Acc : 98.902\n",
      "Epoch : 6 Test Acc : 99.030\n",
      "--------------------------------------------------------------\n",
      "Epoch : 7, Batch : 0, Loss : 0.117 \n",
      "Epoch : 7, Batch : 200, Loss : 0.059 \n",
      "Epoch : 7, Batch : 400, Loss : 0.060 \n",
      "Epoch : 7, Batch : 600, Loss : 0.061 \n",
      "Epoch : 7, Batch : 800, Loss : 0.060 \n",
      "Epoch : 7 Train Acc : 98.802\n",
      "Epoch : 7 Test Acc : 98.870\n",
      "--------------------------------------------------------------\n",
      "Epoch : 8, Batch : 0, Loss : 0.174 \n",
      "Epoch : 8, Batch : 200, Loss : 0.057 \n",
      "Epoch : 8, Batch : 400, Loss : 0.055 \n",
      "Epoch : 8, Batch : 600, Loss : 0.057 \n",
      "Epoch : 8, Batch : 800, Loss : 0.056 \n",
      "Epoch : 8 Train Acc : 99.010\n",
      "Epoch : 8 Test Acc : 98.960\n",
      "--------------------------------------------------------------\n",
      "Epoch : 9, Batch : 0, Loss : 0.051 \n",
      "Epoch : 9, Batch : 200, Loss : 0.046 \n",
      "Epoch : 9, Batch : 400, Loss : 0.048 \n",
      "Epoch : 9, Batch : 600, Loss : 0.050 \n",
      "Epoch : 9, Batch : 800, Loss : 0.050 \n",
      "Epoch : 9 Train Acc : 98.987\n",
      "Epoch : 9 Test Acc : 98.860\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def evalutate(clf, inputs, targets):\n",
    "    clf.eval()\n",
    "    outputs = clf(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = targets.size(0)\n",
    "    correct = predicted.eq(targets.data).cpu().sum()\n",
    "    return total, correct\n",
    "\n",
    "\n",
    "def evalutate_accuracy(clf, loader):\n",
    "    clf.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "        if cuda_available:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        partial_total, partial_correct = evalutate(clf, inputs, targets)\n",
    "        total += partial_total\n",
    "        correct += partial_correct\n",
    "        \n",
    "    return 100*float(correct)/total\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    # Train\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        if cuda_available:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = clf(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.item())\n",
    "        \n",
    "        if batch_idx%200==0:\n",
    "            print('Epoch : %d, Batch : %d, Loss : %.3f ' % (epoch, batch_idx, np.mean(losses)))\n",
    "        \n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = evalutate_accuracy(clf, train_loader)\n",
    "    test_acc = evalutate_accuracy(clf, test_loader)\n",
    "    \n",
    "    print('Epoch : %d Train Acc : %.3f' % (epoch, train_acc))\n",
    "    print('Epoch : %d Test Acc : %.3f' % (epoch, test_acc))\n",
    "    print('--------------------------------------------------------------')\n",
    "    clf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
